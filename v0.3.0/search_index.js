var documenterSearchIndex = {"docs":
[{"location":"#MiniBatchInference.jl","page":"MiniBatchInference.jl","title":"MiniBatchInference.jl","text":"","category":"section"},{"location":"","page":"MiniBatchInference.jl","title":"MiniBatchInference.jl","text":"DocStringExtensions.README","category":"page"},{"location":"","page":"MiniBatchInference.jl","title":"MiniBatchInference.jl","text":"Modules = [MiniBatchInference]","category":"page"},{"location":"#MiniBatchInference.ResultMLE","page":"MiniBatchInference.jl","title":"MiniBatchInference.ResultMLE","text":"Container for ouputs of MLE.\n\nNotes\n\nres = ResultMLE() has all fields empty but res.minloss which is set to Inf.\n\n\n\n\n\n","category":"type"},{"location":"#MiniBatchInference.AIC-Tuple{Any, Any, Any}","page":"MiniBatchInference.jl","title":"MiniBatchInference.AIC","text":"AIC(RSS, k, m)\n\nCalculate AIC of a model given its RSS,  k its number of parameters,  and m the number of observations.\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.AIC-Tuple{ResultMLE, Array, Array}","page":"MiniBatchInference.jl","title":"MiniBatchInference.AIC","text":"AIC(res, data_set, Σ)\n\n\nComputes the AIC of res given the observational noise variance covariance matrix Σ.\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.AIC-Union{Tuple{P}, Tuple{T}, Tuple{T, P}} where {T<:AbstractFloat, P<:Integer}","page":"MiniBatchInference.jl","title":"MiniBatchInference.AIC","text":"AIC(logl, nparams)\n\n\nComputes the AIC given loglikelihood logl and number of parameters nparams.\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.AICc-Tuple{Any, Any, Any}","page":"MiniBatchInference.jl","title":"MiniBatchInference.AICc","text":"AICc(aic, k, m)\n\nCalculate AIC corrected of a model given its aic,  k its number of parameters,  and m the number of observations  (if d variables and T time steps, m = d * T).  The formula is taken from Mangan2017.\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.AICc_TREE-Tuple{Any, Any, Any}","page":"MiniBatchInference.jl","title":"MiniBatchInference.AICc_TREE","text":"AICc_TREE(RSS, k, m)\n\nCalculate aic of a model given its RSS,  k its number of parameters,  and m the number of observations. The formula is taken from TREE article.\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.FIM_strouwen-Tuple{Any, Any, Any}","page":"MiniBatchInference.jl","title":"MiniBatchInference.FIM_strouwen","text":"FIM_strouwen(predict, θ, Σ)\n\nReturns the FIM matrix associated to predict and evaluated at θ taken from https://arnostrouwen.com/post/dynamic-experimental-design/\n\npredict is a function that takes θ as a parameter and returns an array \nwith dim=1 corresponding to state variables and \ndim = 2 corresponding to the time steps\nθ the parameter vector where to evaluate the FIM\nΣ is the variance-covariance matrix of the observation errors\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.FIM_yazdani-NTuple{6, Any}","page":"MiniBatchInference.jl","title":"MiniBatchInference.FIM_yazdani","text":"FIM_yazdani(dudt, u0_true, tspan, p_true, Σ)\n\nReturns the FIM matrix associated to problem defined by prob = ODEProblem(dudt, u0_true, tspan, p_true). Σ is the variance-covariance matrix of the observation errors\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.RSS","page":"MiniBatchInference.jl","title":"MiniBatchInference.RSS","text":"RSS(res, data_set)\nRSS(res, data_set, fn)\n\n\nComputes the RSS of res given data_set.\n\nArgument:\n\nfn is set to identity but can be e.g. log if lognormal loss used.\n\n\n\n\n\n","category":"function"},{"location":"#MiniBatchInference._loss_multiple_shoot_init-Tuple{Any, Any, Any}","page":"MiniBatchInference.jl","title":"MiniBatchInference._loss_multiple_shoot_init","text":"_loss_multiple_shoot_init(data, pred, ic_term)\n\n\ndefault loss function for minibatch_MLE.\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.divisors-Tuple{Any}","page":"MiniBatchInference.jl","title":"MiniBatchInference.divisors","text":"divisors(n)\n\nReturns all divisors of n, sorted.\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.get_u0s-Tuple{ResultMLE}","page":"MiniBatchInference.jl","title":"MiniBatchInference.get_u0s","text":"get_u0s(res)\n\n\nReturns initial condition vector estimated[u_0_1, ..., u_0_n] , where n corresponds to the number of chunks. In the case of independent time series, returns  [[u_0_TS1_1, ..., u_0_TS1_n],...,[u_0_TS1_1,...]], matching the format of res.pred.\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.iterative_minibatch_MLE-Tuple{}","page":"MiniBatchInference.jl","title":"MiniBatchInference.iterative_minibatch_MLE","text":"iterative_minibatch_MLE(; group_sizes, optimizers_array, threshold, kwargs...)\n\n\nPerforms a iterative minibatch MLE, iterating over group_sizes.  Stops the iteration when loss function increases between two iterations.\n\nReturns an array with all ResultMLE obtained during the iteration. For kwargs, see minibatch_MLE.\n\nNote\n\nfor now, does not support independent time series (minibatch_ML_indep_TS).\nat every iteration, initial conditions are initialised given the predition of previous iterations\n\nSpecific arguments\n\ngroup_sizes : array of group sizes to test\noptimizers_array: optimizersarray[i] is an array of optimizers for the trainging processe of `groupsizes[i`\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.loglikelihood-Tuple{ResultMLE, Array, Array}","page":"MiniBatchInference.jl","title":"MiniBatchInference.loglikelihood","text":"loglikelihood(res, data_set, Σ; distrib, fn)\n\n\nComputes the loglikelihood of res given the observational noise variance covariance matrix Σ.\n\nOptions\n\nBy default, normal observational noise is assumed,  but lognormal observational noise can be chosen by setting distrib=MvLogNormal, fn=log\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.minibatch_MLE-Tuple{}","page":"MiniBatchInference.jl","title":"MiniBatchInference.minibatch_MLE","text":"minibatch_MLE(; group_size, kwargs...)\n\n\nMaximum likelihood estimation with minibatching. Loops through ADAM and BFGS. Returns minloss, p_trained, ranges, losses, θs.\n\narguments\n\np_init : initial guess for parameters of prob\ngroup_size : size of segments\ndata_set : data\nprob : ODE problem for the state variables.\ntsteps : corresponding to data\nalg : ODE solver\nsensealg : sensitivity solver\n\noptional\n\nloss_fn : the loss function, that takes as arguments loss_fn(data, pred, ic_term) where    data is the training data and pred corresponds to the predicted state variables.    loss_fn must transform the pred into the observables, with a function    h that maps the state variables to the observables. By default, h is taken as the identity.\nu0_init : if not provided, we initialise from data_set\nloss_fn : loss function with arguments loss_fn(data, pred, ic_term)\nλ : dictionary with learning rates. Dict(\"ADAM\" => 0.01, \"BFGS\" => 0.01)\nmaxiters : dictionary with maximum iterations. Dict(\"ADAM\" => 2000, \"BFGS\" => 1000),\ncontinuity_term : weight on continuity conditions\nic_term : weight on initial conditions\nverbose : displaying loss\ninfo_per_its = 50,\nplotting : plotting convergence loss\ninfo_per_its = 50,\ncb : call back function.   Must be of the form cb(θs, p_trained, losses, pred, ranges)\np_true : true params\np_labs : labels of the true parameters\nthreshold : default to 1e-6\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.minibatch_ML_indep_TS-Tuple{}","page":"MiniBatchInference.jl","title":"MiniBatchInference.minibatch_ML_indep_TS","text":"minibatch_ML_indep_TS(; group_size, data_set, tsteps, kwargs...)\n\n\nSimilar to minibatch_MLE but for independent time series, where data_set is a vector containing the independent arrays corresponding to the time series, and tsteps is a vector where each entry contains the time steps of the corresponding time series.\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.minibatch_loss-Tuple{AbstractArray, AbstractArray, AbstractArray, SciMLBase.ODEProblem, Any, Any, SciMLBase.AbstractODEAlgorithm, AbstractArray}","page":"MiniBatchInference.jl","title":"MiniBatchInference.minibatch_loss","text":"minibatch_loss(θ, ode_data, tsteps, prob, loss_function, continuity_loss, solver, ranges; continuity_term, kwargs...)\n\n\nReturns a tuple (loss, pred) obtained from minibatching of the  time series ode_data into segments with time steps given by tsteps[ranges[i]]. The initial conditions are assumed free parameters for each segments.\n\nArguments:\n\nθ: [u0,p] where p corresponds to the parameters of ode function.\node_data: Original Data to be modelled.\ntsteps: Timesteps on which ode_data was calculated.\nprob: ODE problem that the Neural Network attempts to solve.\nloss_function: Any arbitrary function to calculate loss.\ncontinuity_loss: Function that takes states hatu_end of group k and\n\nu_0 of group k+1 as input and calculates prediction continuity loss   between them.   If no custom continuity_loss is specified, sum(abs, û_end - u_0) is used.\n\nsolver: ODE Solver algorithm.\nranges: Vector containg range for each segment.\ncontinuity_term: Weight term to ensure continuity of predictions throughout different groups.\nkwargs: Additional arguments splatted to the ODE solver. Refer to the\n\nLocal Sensitivity Analysis and   Common Solver Arguments   documentation for more details.\n\nNote:\n\nThe parameter 'continuity_term' should be a relatively big number to enforce a large penalty whenever the last point of any group doesn't coincide with the first point of next group.\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.moments!-Tuple{Any, Any}","page":"MiniBatchInference.jl","title":"MiniBatchInference.moments!","text":"moments!(xx, x)\n\nreurns the moments of x as a vector stored in xx\n\nArgs\nxx : the covariate vector\nx :  the state variable vector\n\n\n\n\n\n","category":"method"},{"location":"#MiniBatchInference.moments-Tuple{Any}","page":"MiniBatchInference.jl","title":"MiniBatchInference.moments","text":"moments(x)\n\nreurns the moments of x as a vector\n\nArgs\nxx : the covariate vector\nx :  the state variable vector\n\n\n\n\n\n","category":"method"}]
}
